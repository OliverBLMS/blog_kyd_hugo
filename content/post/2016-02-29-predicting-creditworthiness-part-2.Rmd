---
title: 'Predicting creditworthiness: part-2'
author: Oliver Belmans
date: '2016-02-29'
slug: predicting-creditworthiness-part-2
categories:
  - R
tags:
  - Predictive
  - banking industry
  - decision tree
  - Cross validation
  - Boosting
  - Bagging
  - Random Forest
banner: ''
description: ''
images: []
menu: ''
---

<!--more-->


```{r c loading data, message=FALSE, warning=FALSE, include=FALSE}
# Used packages
require(dplyr)      # for most data manipulation
require(caret)      # acces to alot of predictors
require(ggplot2)    # nice plots
require(gridExtra)  # multiplots
require(ROCR)       # determing performance and ROC plot
require(C50)

# Loading data
credit <- 
  read.csv("datasets/german_credit.csv", stringsAsFactors = FALSE) 
# Remove . dots in names
names(credit) <- 
  gsub(pattern = "\\.", x = names(credit), replacement = "")
# Make target class factor
credit$Creditability <- 
  factor(credit$Creditability, levels = c(1, 0), labels = c("Good", "Bad"))

# Use seed to ensure consistency in sampling
set.seed(2)
# Split train/test data using the carret::createDataPartition function
# note: other methods exist like base R, or dplyr
TrainPart <- 
  createDataPartition(y = credit$Creditability, 
                      times = 1, 
                      p = 0.7, 
                      list = FALSE)
training = credit[TrainPart,]
testing = credit[-TrainPart,]
```

## Refining the credit model(s)

To continue with the creditworthiness case, I want to explore this case a little bit more by adding more meta algorithms such as boosting, winnowing, cross validation etc. Additionally, I'll use `randomforest` as classifier algorithm. 

I'm still using the same german credit data as in the previous post. I'm also using the same train/testest. Each model is stored into one object `models`.

```{r, echo=TRUE}
# object that will store all the models in a list
models <- list()
```

I start with three different models, which are all generated with the `C5.0` algorithm. __First model__ is a default model with no extra features. __Second model__ is amplified with the boosting feature: instead of generating just one classifier it will generate several classifiers. After each iteration it will focus more on misclassified examples for reducing bias. The __third model__ has the `winnow` parameter set to `TRUE`. Basicly, it will search over the 20 attributes of the dataset and pre-select a subset of attribute that will be used to construct the decision tree or ruleset. Read more at [C5.0 tutorial](http://www.rulequest.com/see5-unix.html#WINNOWING)

```{r}
# C5.0 package
set.seed(2)
# train model
baseMod <- 
  C5.0(
    training[,-1],
    training$Creditability)
# store basemodel into the models object
models$baseMod <- baseMod

# Using boosting with C5.0 model
set.seed(2)
# train model
BoostMod <- 
  C5.0(
    training[,-1],
    training$Creditability,
    trials = 100)
# store boostmod into the models object
models$BoostMod <- BoostMod

# Using winnow and boosting
set.seed(2)
# train model
WinnowMod <- 
  C5.0(
    training[,-1],
    training$Creditability,
    control = C5.0Control(winnow = TRUE),
    trials = 100)
# store winnowmod into the models object
models$WinnowMod <- WinnowMod

```

So the models created thus far:
```{r}
names(models)
```

## Performance measures

After training, lets gather the performance of those models on new examples. With the `ROCR` package we can do lots of performance tests such as: Area under the Curve, sensitivity, specificity, accuracy, etc. I made a few functions `accuracyTester`, `getPerformance`, `getSensSpec` and `getVarImportance` so I can run those function for each model in the `models` object.

```{r, echo=TRUE}
# function for returning accuracy on test dataset
accuracyTester <- 
  function(predictModel) {
    temp <- predict(predictModel, testing)
    postResample(temp, testing$Creditability)
  }
# function for calculating performance
# input for the ROC curve
getPerformance <- 
  function(modelName) {
    score <- predict(modelName, type= "prob", testing)
    pred <-  prediction(score[,1], testing$Creditability)
    perf <- performance(pred, "tpr", "fpr")
    return(perf)
  }
# function for calculating specificity and sensitivity
getSensSpec <- 
  function(modelName) {
    score <- predict(modelName, type= "prob", testing)
    pred <-  prediction(score[,1], testing$Creditability)
    perf <- performance(pred, "sens", "spec")
    return(perf)
  }
```

### ROC and Accuracy plot

One method to evaluate the models is by calculating the overall accuracy of each model. The `BoostMod` has the highest accuracy.

```{r, echo=FALSE,  message=FALSE, warning=FALSE, fig.align='center'}
plotcolor <- rainbow(5)
# accuracy function
accuracy <- vector(mode='numeric')
for (i in names(models)){
  accuracy[i] <- accuracyTester(get(i))
}
dotchart(sort(accuracy), main = "Accuracy of models on test data", color = plotcolor[1:3], cex = 0.7, bg = plotcolor[1:3])
```


A more reliable method to evaluate the model's performace is the Receiver Operating Characteristics. It's a well used visualization technique to evaluate binary classifiers. Predicting good or bad creditworthiness is indeed a binary classification. A ROC curve is created by plotting the true positive rate (TPR) or sensitivity against the false positive rate (FPR), thus it shows the tpr as a function of fpr. 

For each fpr it is shown that the `BoostMod` has the highest tpr. 

```{r, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
modelPerf <- lapply(models, function(x) getPerformance(x))
plot(modelPerf$baseMod, col = plotcolor[1], lty = 1, lwd= 1, main = "ROC for all the prediction models")
plot(modelPerf$BoostMod, add = TRUE, col = plotcolor[2], lty = 1, lwd= 1)
plot(modelPerf$WinnowMod, add = TRUE, col = plotcolor[3], lty = 1, lwd= 1)
legend(0.7,0.3 ,legend = c("baseMod", "BoostMod", "WinnowMod"), lwd = 3, col = plotcolor, cex = 0.7)

```


## Caret package

Another great package I found is the `caret` package. It has an uniform interface to a lot of [predictive algorithms](https://topepo.github.io/caret/modelList.html). Also, it provides a generic approach for visualization, pre-processing, data-splitting, variable importance, model performance and parallel processing. This can be handy, since different modeling functions have different syntax for model training, predicting and parameter tuning.

`Caret` has bindings to the `C5.0` algorithm, therefor it will also tune the parameters boosting and winnowing. 
Another way to get a more reliable estimate of accuracy is by [__K-fold cross-validation__](http://genome.tugraz.at/proclassify/help/pages/XV.html). Just for illustration I will use a 10-fold cross validation, but will use it only on our training set. So I can use the testset for the other performance measures.

This image illustrates the mechanics of cross-validation:
![10-fold cross-validation](https://images.duckduckgo.com/iu/?u=http%3A%2F%2Fchrisjmccormick.files.wordpress.com%2F2013%2F07%2F10_fold_cv.png&f=1)

```{r, message=FALSE, warning=FALSE}
# a list of values that define how this function acts
ctrl <- 
  trainControl(method = 'repeatedcv', # 10-fold cv
               number=10,             # 10-fold cross-validation
               repeats=5)             # 5-repeats 
# train model
set.seed(2)
cvMod <- 
  train(form = Creditability ~.,
        data = training,
        method     = "C5.0",
        trControl  = ctrl,
        tuneGrid = expand.grid(trials = 15, model = c("tree", "rules"), winnow = c(T,F)))
# store rfmodel into the models object
models$cvMod <- cvMod
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}

accuracy <- vector(mode='numeric')
for (i in names(models)){
  accuracy[i] <- accuracyTester(get(i))
}
dotchart(sort(accuracy), main = "Accuracy of models on test data", color = plotcolor[1:5], cex = 0.8, bg = plotcolor[1:5])

```

So far I started exploring the construction of a single classification tree with the `C5.0` packages. I tried to improve the performance by adding an ensemble learner (boosting). Looking at the ROC and Accuracy plot this seems to be the best performing model so far. Another ensemble learner can be done for example with the `randomForest` package. Intead of using boosting or cross-validation it will use another technique called [__bagging__](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#intro) (__b__ootstrap __agg__regat__ing__). 

Here, I'm only learning a forest on the training set, so I can evaluate its performance just like the other models. 

```{r, message=FALSE, warning=FALSE}
library(randomForest)
# RANDOMFOREST
set.seed(2)
rfModel <- 
  randomForest(form = Creditability ~., data = training,
               ntree=500, 
               importance=T, 
               proximity=T,
               keep.forest = TRUE
  )
# store rfmodel into the models object
models$rfModel <- rfModel
```

A slight improvement can be seen on the ROC curve. The model with cross-validation and randomforest are slightly higher on the curve.

```{r, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
modelPerf <- lapply(models, function(x) getPerformance(x))
# plot ROC for each model / add = true to overlay plots
plot(modelPerf$baseMod, col = plotcolor[1], lty = 1, lwd= 1,  main = "ROC for all the prediction models")
plot(modelPerf$BoostMod, add = TRUE, col = plotcolor[2], lty = 1, lwd= 1)
plot(modelPerf$WinnowMod, add = TRUE, col = plotcolor[3], lty = 1, lwd= 1)
plot(modelPerf$cvMod, add = TRUE, col = plotcolor[4], lty = 1, lwd= 1)
plot(modelPerf$rfModel, add = TRUE, col = plotcolor[5], lty = 1, lwd = 1)
legend(0.5,0.4 ,legend = c("baseMod", "BoostMod", "WinnowMod", "Cross-Validation Caret", "RandomForest"), lwd = 3, col = plotcolor, cex = 0.7)

```

Another way to evaluate the ROC performance can be done by calculating the __Area under the Curve__. Again, both previous best models have the same and highest AUC.

```{r, echo=FALSE}

getAUC <-
  function(modelName){
    score <- predict(modelName, type= "prob", testing)
    pred <-  prediction(score[,1], testing$Creditability)
    perf <- performance(pred, "auc")
    AUC = round(perf@y.values[[1]], 2)
    return(AUC)

  }
modelsAUC <- 
  lapply(models, function(x) getAUC(x))
knitr::kable(data.frame(modelsAUC),format = "html")
```

The accuracy of rfModel has the highest accuracy, but at what cost?

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
accuracy <- vector(mode='numeric')
for (i in names(models)){
  accuracy[i] <- accuracyTester(get(i))[1]
  # print(i)
}
dotchart(sort(accuracy), main = "Accuracy of models on test data", color = plotcolor[1:5], cex = 0.8, bg = plotcolor[1:5])
```

I guess a bank will choose a more conservative approach and follows a strategy with a more precise prediction for bad creditworthiness. Thus, a bank will prefer to avoid more false positives (predicted good, actual bad) than false negatives (predicted bad, actual true). So I would assume banks will likely choose a model with a good specificity. 

Given this notion a bank can evaluate its options by looking at both:

* sensitivity = $number of true positives \over number of true positives + number of false negatives$ 
* specificity = $number of true negatives \over number of true negatives + number of false positives$

For each given model:

```{r, echo=FALSE}
modelSensSpec <- lapply(models, function(x) getSensSpec(x))

getBestCutoff <- function(foo){
  cutOff <- 
    data.frame(cut=foo@alpha.values[[1]], 
               sens=foo@y.values[[1]],
               spec=foo@x.values[[1]])
  bestCutOff <-
    cutOff[which.max(cutOff$sens + cutOff$spec), "cut"]
  temp <- round(cutOff[cutOff$cut == bestCutOff,],2)
  return(temp)
}
modelCutOff <- 
  lapply(modelSensSpec, function(x) getBestCutoff(x))
knitr::kable(do.call(rbind.data.frame, modelCutOff), format = "html")
```



