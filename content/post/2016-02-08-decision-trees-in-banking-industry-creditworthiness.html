---
title: 'Decision trees in banking industry: creditworthiness'
author: Oliver Belmans
date: '2016-02-08'
slug: decision-trees-in-banking-industry-creditworthiness
categories:
  - R
tags:
  - decision tree
  - Predictive
  - banking industry
  - Information gain
  - Entropy
banner: ''
description: ''
images: []
menu: ''
---



<!--more-->
<div id="predicting-creditability" class="section level2">
<h2>Predicting creditAbility</h2>
<p>While looking for a interesting Machine Learning exercise I decided to go along with credit scoring exercise. I want to know what kind of information influences the decision for giving someone credit (or not). Typically, a bank would ask you to fill in some kind of assesment form with question about demographics, purpose of the loan, your status of employment and salary. Today, this is not a standard proces.</p>
<p>The problem here is we want to predict the creditability of new clients based on clients from the past. Client whom we already know if they are credit-worthy or not. One way to solve this problem is with a machine learning such as decision trees. Other algorithms can be used aswell, but decision trees are more comprehensive at the end as I want to know what kind of information has the most influence in this determination proces.</p>
<p>In this example we already know which client are credit-worthy and which are not (= qualitative characteristic), based on 21 quantitative characteristics. All we have to do is to find (alot of) tests/decisions that partitions those clients as well as possible. Hereby, we want decisions that at the end group the good or bad credit-worthy clients as <strong>pure</strong> as possible. For each possible decisions (or split in the decision tree) we can define a measure of expected purity. During these path of decisions we follow those decisisons with the most <strong>Information Gain</strong>, thus reducing the missing information the most.</p>
<div id="get-the-data" class="section level3">
<h3>Get the data</h3>
<p>The dataset I’m using has already been preprocessed. If you would like the original dataset take a look at the <a href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29">UCI Machine Learning Repository</a>. The dataset holds both numeric and categoric variables, but has been mapped to only numeric variables. See the <a href="https://onlinecourses.science.psu.edu/stat857/node/222">appendix here</a> for the final categorical classification.</p>
<pre class="r"><code># Used packages
require(dplyr)      # for most data manipulation
require(caret)      # acces to alot of predictors
require(ggplot2)    # nice plots
require(gridExtra)  # multiplots
# Loading data
credit &lt;- 
  read.csv(&quot;datasets/german_credit.csv&quot;, stringsAsFactors = FALSE) 
# Remove . dots in names
names(credit) &lt;- 
  gsub(pattern = &quot;\\.&quot;, x = names(credit), replacement = &quot;&quot;)
# Make target class factor
credit$Creditability &lt;- 
  factor(credit$Creditability, levels = c(1, 0), labels = c(&quot;Good&quot;, &quot;Bad&quot;))</code></pre>
</div>
<div id="exploring-the-data" class="section level3">
<h3>Exploring the data</h3>
<p>Quick overview of few characteristics to get an idea of the dataset:</p>
<pre class="r"><code># dimension: 20 quant. characteristics and 1 target class (creditability)
dim(credit)</code></pre>
<pre><code>## [1] 1000   21</code></pre>
<pre class="r"><code># show few records
credit[18:24, 1:6]</code></pre>
<pre><code>##    Creditability AccountBalance DurationofCreditmonth
## 18          Good              2                    18
## 19          Good              2                    36
## 20          Good              4                    11
## 21          Good              1                     6
## 22          Good              2                    12
## 23           Bad              2                    36
## 24          Good              2                    12
##    PaymentStatusofPreviousCredit Purpose CreditAmount
## 18                             2       3         3213
## 19                             4       3         2337
## 20                             4       0         7228
## 21                             4       0         3676
## 22                             4       0         3124
## 23                             2       5         2384
## 24                             4       4         1424</code></pre>
<p>A bivariate visualization with summary statistics can provide usefull insights of the dataset, as well at the target variable. Most people asking for a credit is the group between 20-40 years but it not so obvious to decide which age is better to get a credit or not.</p>
<p><img src="/post/2016-02-08-decision-trees-in-banking-industry-creditworthiness_files/figure-html/c%20data%20exploration-1.png" width="864" /></p>
<p>This multivariate visualizations of only a few variables shows that it’s not that simple to group the classes of creditability (good/bad). All points lie upon each other for the characteristics credit amount, duration of credit and age.</p>
<pre><code>## Loading required package: AppliedPredictiveModeling</code></pre>
<p><img src="/post/2016-02-08-decision-trees-in-banking-industry-creditworthiness_files/figure-html/matrix%20plot-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>Let’s look at the summary of the qualitative characteristic</p>
<pre class="r"><code>summary(credit$Creditability)</code></pre>
<pre><code>## Good  Bad 
##  700  300</code></pre>
<p>The dataset is slightly unbalanced to learn from.</p>
</div>
<div id="preparing-the-data" class="section level3">
<h3>Preparing the data</h3>
<p>Before training machine learning algorithms we need to split up the dataset into a training- and testset. This can be done with <code>base R</code>, <code>dplyr</code> or with <code>caret</code>’s function <code>createDataPartition</code>, which will take a stratified sample. Thus, respecting the distribution of our target label in both sets.</p>
<pre class="r"><code># Split full dataset into a training and test set
set.seed(2)
# dplyr
credit = sample_n(credit, size = nrow(credit))
training = sample_frac(credit, size =0.7)
testing = setdiff(credit, training)

# caret
TrainPart &lt;- createDataPartition(y = credit$Creditability, 
                    times = 1, 
                    p = 0.7, 
                    list = FALSE)
training = credit[TrainPart,]
testing = credit[-TrainPart,]</code></pre>
</div>
<div id="training-a-model" class="section level3">
<h3>Training a model</h3>
<p>Very popular classification algorithms are <code>C4.5</code> and <code>C5.0</code>. For this problem I choose the <code>C5.0</code> because it incorporates a misclassification cost and <code>C4.5</code> doesn’t have this feature. This mean that I don’t want all errors treated as equal but more like this: If the model predicts “good credithworthiness” but it is actual a “bad” one, these errors should get a higher cost in order to avoid them while learning our decision tree.</p>
<div id="train" class="section level4">
<h4>Train</h4>
<p>Our model has learned some ruleset that can be visualized as a tree. Note that not all leaves (terminal nodes) are pure! <img src="/post/2016-02-08-decision-trees-in-banking-industry-creditworthiness_files/figure-html/unnamed-chunk-3-1.png" width="864" /></p>
<p>We can also take a look at the overal predictor importance, since I’m mostly interested in this.</p>
<pre class="r"><code># ?C5imp
C5varImp &lt;- C5imp(credit_model, metric = &quot;usage&quot;)
C5varImp</code></pre>
<pre><code>##                               Overall
## AccountBalance                 100.00
## Typeofapartment                 59.29
## DurationofCreditmonth           58.14
## PaymentStatusofPreviousCredit   53.57
## Guarantors                      51.14
## Mostvaluableavailableasset      48.86
## ConcurrentCredits               43.86
## CreditAmount                    39.14
## Lengthofcurrentemployment       38.86
## Purpose                         32.86
## Occupation                      26.57
## SexMaritalStatus                23.00
## Instalmentpercent               15.57
## ValueSavingsStocks              12.00
## DurationinCurrentaddress        11.29
## Ageyears                         8.57
## Telephone                        8.43
## Noofdependents                   5.86
## NoofCreditsatthisBank            3.71
## ForeignWorker                    3.71</code></pre>
<p>More details about the model are shown with <code>summary(credit_model)</code>. The model has an error rate of 6.9% (48/700 examples misclassified). A total of 34 actual bad creditability were incorrectly classified as good credithworthiness (false positives), while 14 good creditability were misclassified as bad ones (false negatives). The error rate on the training data is probably very optimistic, so lets evaluate this on the testset.</p>
</div>
</div>
<div id="evaluating-model-performance-on-new-examples" class="section level3">
<h3>Evaluating model performance on new examples</h3>
<pre class="r"><code># run predict model on new examples
predClass &lt;- 
  predict.C5.0(credit_model, testing) 
# Confusion matrix
require(gmodels)</code></pre>
<pre><code>## Loading required package: gmodels</code></pre>
<pre class="r"><code>CrossTable(testing$Creditability, predClass, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;))</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  300 
## 
##  
##                | predicted default 
## actual default |      Good |       Bad | Row Total | 
## ---------------|-----------|-----------|-----------|
##           Good |       173 |        37 |       210 | 
##                |     0.577 |     0.123 |           | 
## ---------------|-----------|-----------|-----------|
##            Bad |        55 |        35 |        90 | 
##                |     0.183 |     0.117 |           | 
## ---------------|-----------|-----------|-----------|
##   Column Total |       228 |        72 |       300 | 
## ---------------|-----------|-----------|-----------|
## 
## </code></pre>
<p>On the testset a total of 92/300 cases are misclassified. Thus, this gives us an error rate of 30%. This is a lot higher than the 6.9% error rate from the traininset. The model is about 70% accurate… However, this metric is not ideal to use with unbalanced data, but gives a sense of the overall prediction.</p>
</div>
<div id="methods-to-improve-the-model-boosting" class="section level3">
<h3>Methods to improve the model: Boosting</h3>
<p>Boosting algorithms consist of iteratively learning weak classifiers and adding them together to end with a final strong classifier. During each iteration, weights are given to those examples that are misclassified whereas examples lose weights when classified correctly. Thus, the next iteration focusses more on the examples that previously were misclassified.<br />
The <code>C5.0</code> algorithm has a boost functionality that can be controlled with the <code>trails</code> parameter.</p>
<pre class="r"><code>credit_boost &lt;- 
  C5.0(training[-1], 
  training$Creditability, 
  trials = 15) # an integer specifying the number of boosting iterations

# summary(credit_boost)
boostPred &lt;-
  predict.C5.0(credit_boost, testing, trials = credit_boost$trials[&quot;Actual&quot;]) 
# Confusio matrix
CrossTable(testing$Creditability, boostPred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;))</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  300 
## 
##  
##                | predicted default 
## actual default |      Good |       Bad | Row Total | 
## ---------------|-----------|-----------|-----------|
##           Good |       180 |        30 |       210 | 
##                |     0.600 |     0.100 |           | 
## ---------------|-----------|-----------|-----------|
##            Bad |        47 |        43 |        90 | 
##                |     0.157 |     0.143 |           | 
## ---------------|-----------|-----------|-----------|
##   Column Total |       227 |        73 |       300 | 
## ---------------|-----------|-----------|-----------|
## 
## </code></pre>
<p>Error rate (on test set) in now down to 25.6%. Only a minor improvement is achieved with boosting.</p>
</div>
<div id="costmatrix" class="section level3">
<h3>Costmatrix</h3>
<p>Let’s implement a cost matrix so that not all errors are treated as equal but implements asymmetric costs for specific errors. First, we need to define a cost matrix and specify the dimension. We need a 2x2 cost matrix since the predicted and actual values form a 2x2 matrix.</p>
<pre class="r"><code># Make a cost matrix
matrix_dimensions &lt;- list(c(&quot;Bad&quot;, &quot;Good&quot;), c(&quot;Bad&quot;, &quot;Good&quot;))
names(matrix_dimensions) &lt;- c(&quot;predicted&quot;, &quot;actual&quot;)
# Assing a costmatrix to error_cost
error_cost &lt;- matrix(c(0, 4, 1, 0), nrow = 2, dimnames = matrix_dimensions)
# Use the costmatrix and train the model again
credit_cost &lt;- 
  C5.0(training[-1], 
       training$Creditability, 
       costs = error_cost,
       trails = 10)</code></pre>
<p>Take a look at the confusion matrix:</p>
<pre class="r"><code>credit_cost_pred &lt;- predict.C5.0(credit_cost, testing, type = &quot;class&quot;)

CrossTable(testing$Creditability, credit_cost_pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c(&#39;actual default&#39;, &#39;predicted default&#39;))</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  300 
## 
##  
##                | predicted default 
## actual default |      Good |       Bad | Row Total | 
## ---------------|-----------|-----------|-----------|
##           Good |       121 |        89 |       210 | 
##                |     0.403 |     0.297 |           | 
## ---------------|-----------|-----------|-----------|
##            Bad |        24 |        66 |        90 | 
##                |     0.080 |     0.220 |           | 
## ---------------|-----------|-----------|-----------|
##   Column Total |       145 |       155 |       300 | 
## ---------------|-----------|-----------|-----------|
## 
## </code></pre>
<pre class="r"><code>postResample(credit_cost_pred, testing$Creditability)</code></pre>
<pre><code>##  Accuracy     Kappa 
## 0.6233333 0.2565789</code></pre>
<p>This model has lower accuracy (62.3%) with more mistakes. However, the type of errors are very different. This time it will try to reduce the costly predictions (actual bad creditability but predicted as good).</p>
<div id="to-do" class="section level4">
<h4>To-do</h4>
<ul>
<li>try other methods like <code>randomForest</code>, explore the <code>caret</code> function</li>
<li>try other meta-algorithms like bagging, cross-validation</li>
</ul>
</div>
</div>
</div>
